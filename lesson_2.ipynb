{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c784205-e505-45d0-9a13-f8d857756968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2895c587-eca9-4da7-a1f7-283ea06ba6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../lesson_1/combine_df.pkl')\n",
    "df  = df[['tweet_stemmed', 'tweet_lemitized','label']]\n",
    "df = df.loc[df['label']>=0]\n",
    "df['tweet_stemmed']=df['tweet_stemmed'].apply(lambda x: \" \".join(x) )\n",
    "df['tweet_lemitized']=df['tweet_lemitized'].apply(lambda x: \" \".join(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79cfd24-d1ec-417d-baf1-c41d68417e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9666788b-e6ba-45a3-9123-8569a4caf3fb",
   "metadata": {},
   "source": [
    "### 1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b719d7a-b62a-4e6b-9ada-db6b06245ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64208b66-229e-4878-8ff7-42ac028d6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='word', binary=False, max_df= 0.9, max_features = 1000, stop_words='english')\n",
    "bag_of_words = count_vectorizer.fit_transform(df['tweet_stemmed'])\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_cv_stemed = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370b6a2d-5498-46e9-a5d8-76bfafc26d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count_vectorizer.fit_transform(df['tweet_lemitized'])\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_cv_lemitized = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89be73b4-0bc4-4eee-b4ad-f5d2855841ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=False, max_df= 0.9, max_features = 1000, stop_words='english')\n",
    "bag_of_words = count_vectorizer.fit_transform(df['tweet_stemmed'])\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_cv_stemed_trigram = pd.DataFrame(bag_of_words.toarray(), columns =feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478303f1-5cca-47b9-b136-087070db065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count_vectorizer.fit_transform(df['tweet_lemitized'])\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_cv_lemitized_trigram = pd.DataFrame(bag_of_words.toarray(), columns =feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3982940e-2f88-4b68-a238-983020debcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=False, max_df= 0.9,min_df=0.01 ,max_features = 1000, stop_words='english')\n",
    "bag_of_words = count_vectorizer.fit_transform(df['tweet_stemmed'])\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_cv_stemed_trigram_min_df = pd.DataFrame(bag_of_words.toarray(), columns =feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c8af40d-fcdc-4214-93c4-298fba729629",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count_vectorizer.fit_transform(df['tweet_lemitized'])\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_cv_lemitized_trigram_min_df = pd.DataFrame(bag_of_words.toarray(), columns =feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fedced02-9135-46d7-95ca-5bdce6f120af",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=False, max_df= 0.9,min_df=0.01 ,max_features = 10000, stop_words='english')\n",
    "bag_of_words = count_vectorizer.fit_transform(df['tweet_stemmed'])\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_cv_stemed_trigram_min_df_many_features = pd.DataFrame(bag_of_words.toarray(), columns =feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "139e4e0e-0752-47bf-8970-c51cf78e6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count_vectorizer.fit_transform(df['tweet_lemitized'])\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_cv_lemitized_trigram_min_df_many_features = pd.DataFrame(bag_of_words.toarray(), columns =feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4989c8-29fa-4148-b68f-06b1fe903cb7",
   "metadata": {},
   "source": [
    "### 2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "542676a3-fee5-4caf-a142-769dee4f2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), analyzer='word', binary=False, max_df= 0.9, max_features = 1000, stop_words='english')\n",
    "bag_of_words = tf_idf_vectorizer.fit_transform(df['tweet_stemmed'])\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "df_tf_idf_stemed = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e43fe9-db9a-4c2e-9f90-584cdea9b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = tf_idf_vectorizer.fit_transform(df['tweet_lemitized'])\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "df_tf_idf_lemitized = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343cf4e8-9a4d-41d0-a795-6a7ed601364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False, max_df= 0.9, max_features = 1000, stop_words='english')\n",
    "bag_of_words = tf_idf_vectorizer.fit_transform(df['tweet_stemmed'])\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "df_tf_idf_stemed_trigram = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ae7c40-dbb9-469f-9666-5123cfe10fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = tf_idf_vectorizer.fit_transform(df['tweet_lemitized'])\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "df_tf_idf_lemitized_trigram = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6537dd1-ca37-4edb-ad6b-a0398e932f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False, max_df= 0.9,min_df=0.01 ,max_features = 1000, stop_words='english')\n",
    "bag_of_words = tf_idf_vectorizer.fit_transform(df['tweet_stemmed'])\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "df_tf_idf_stemed_trigram_min_df = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49a1f933-cd69-4797-9ff6-992a84645338",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = tf_idf_vectorizer.fit_transform(df['tweet_lemitized'])\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "df_tf_idf_lemitized_trigram_min_df = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f282eab3-4c49-4353-bee8-6c184bea766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False, max_df= 0.9,min_df=0.01 ,max_features = 10000, stop_words='english')\n",
    "bag_of_words = tf_idf_vectorizer.fit_transform(df['tweet_stemmed'])\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "df_tf_idf_stemed_trigram_min_df_many_features = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2482f197-2271-44c9-9561-90c6da5d8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = tf_idf_vectorizer.fit_transform(df['tweet_lemitized'])\n",
    "feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "df_tf_idf_lemitized_trigram_min_df_many_features = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cafa51f-8d35-445a-ba1f-366d729b89b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddca5d38-2229-49e2-889d-1ec1cbfeae2e",
   "metadata": {},
   "source": [
    "### 3. Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора) обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, так же попробуйте применить к векторайзерам PCA для сокращения размерности посмотрите на качество сделайте выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bd349-a482-4687-9de2-0c433c4d7b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e504f4b-60e1-463a-be3e-79950919b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "result = {}\n",
    "\n",
    "def model_learn (df_preproceser : pd.DataFrame, preprocessor : str, pca_ : bool) -> pd.DataFrame:\n",
    "    \n",
    "    if pca_ == True :\n",
    "        pca = PCA(n_components=77)\n",
    "        df_preproceser = pca.fit_transform(df_preproceser)\n",
    "        df_preproceser = pd.DataFrame(data = df_preproceser, columns=[['1','2','3','4','5','6','7','8','9','10',\n",
    "                                                                      '11','12','13','14','15','16','17','18','19','20',\n",
    "                                                                       '21','22','23','24','25','26','27','28','29','30',\n",
    "                                                                       '31','32','33','34','35','36','37','38','39','40',\n",
    "                                                                       '41','42','43','44','45','46','47','48','49','50',\n",
    "                                                                       '51','52','53','54','55','56','57','58','59','60',\n",
    "                                                                       '61','62','63','64','65','66','67','68','69','70',\n",
    "                                                                       '71','72','73','74','75','76','77'\n",
    "                                                                      ]] )\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_preproceser, df['label'])\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    classifier = linear_model.LogisticRegression(random_state=1)\n",
    "    classifier.fit(train_x, train_y)\n",
    "    predictions = classifier.predict(valid_x)\n",
    "    result[f'logreg_task_params_{preprocessor}'] =  (accuracy_score(valid_y, predictions))\n",
    "    #pd.concat(df_result[f'logreg_task_params_{preprocessor}'],) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c8c067d-94fd-4ff4-a9a8-b3d1c7944505",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_learn(df_cv_stemed_trigram_min_df_many_features, 'df_cv_stemed_trigram_min_df_many_features_pca', True)\n",
    "model_learn(df_cv_stemed_trigram_min_df_many_features, 'df_cv_stemed_trigram_min_df_many_features_none_pca', False)\n",
    "model_learn(df_cv_lemitized_trigram_min_df_many_features, 'df_cv_lemitized_trigram_min_df_many_features_pca', True)\n",
    "model_learn(df_cv_lemitized_trigram_min_df_many_features, 'df_cv_lemitized_trigram_min_df_many_features_none_pca', False)\n",
    "model_learn(df_tf_idf_stemed_trigram_min_df_many_features, 'df_tf_idf_stemed_trigram_min_df_many_features_pca', True)\n",
    "model_learn(df_tf_idf_stemed_trigram_min_df_many_features, 'df_tf_idf_stemed_trigram_min_df_many_features_none_pca', False)\n",
    "model_learn(df_tf_idf_lemitized_trigram_min_df_many_features, 'df_tf_idf_lemitized_trigram_min_df_many_features_pca', True)\n",
    "model_learn(df_tf_idf_lemitized_trigram_min_df_many_features, 'df_tf_idf_lemitized_trigram_min_df_many_features_none_pca', False)\n",
    "model_learn(df_cv_stemed_trigram_min_df, 'df_cv_stemed_trigram_min_df_pca', True)\n",
    "model_learn(df_cv_stemed_trigram_min_df, 'df_cv_stemed_trigram_min_df_none_pca', False)\n",
    "model_learn(df_cv_lemitized_trigram_min_df, 'df_cv_lemitized_trigram_min_df_pca', True)\n",
    "model_learn(df_cv_lemitized_trigram_min_df, 'df_cv_lemitized_trigram_min_df_none_pca', False)\n",
    "model_learn(df_tf_idf_stemed_trigram_min_df, 'df_tf_idf_stemed_trigram_min_df_pca', True)\n",
    "model_learn(df_tf_idf_stemed_trigram_min_df, 'df_tf_idf_stemed_trigram_min_df_none_pca', False)\n",
    "model_learn(df_tf_idf_lemitized_trigram_min_df, 'df_tf_idf_lemitized_trigram_min_df_pca', True)\n",
    "model_learn(df_tf_idf_lemitized_trigram_min_df, 'df_tf_idf_lemitized_trigram_min_df_none_pca', False)\n",
    "model_learn(df_cv_stemed_trigram, 'df_cv_stemed_trigram_pca', True)\n",
    "model_learn(df_cv_stemed_trigram, 'df_cv_stemed_trigram_none_pca', False)\n",
    "model_learn(df_cv_lemitized_trigram, 'df_cv_lemitized_trigram_pca', True)\n",
    "model_learn(df_cv_lemitized_trigram, 'df_cv_lemitized_trigram_none_pca', False)\n",
    "model_learn(df_tf_idf_stemed_trigram, 'df_tf_idf_stemed_trigram_pca', True)\n",
    "model_learn(df_tf_idf_stemed_trigram, 'df_tf_idf_stemed_trigram_none_pca', False)\n",
    "model_learn(df_tf_idf_lemitized_trigram, 'df_tf_idf_lemitized_trigram_pca', True)\n",
    "model_learn(df_tf_idf_lemitized_trigram, 'df_tf_idf_lemitized_trigram_none_pca', False)\n",
    "model_learn(df_cv_stemed, 'df_cv_stemed_pca', True)\n",
    "model_learn(df_cv_stemed, 'df_cv_stemed_none_pca', False)\n",
    "model_learn(df_cv_lemitized, 'df_cv_lemitized_pca', True)\n",
    "model_learn(df_cv_lemitized, 'df_cv_lemitized_none_pca', False)\n",
    "model_learn(df_tf_idf_stemed, 'df_tf_idf_stemed_pca', True)\n",
    "model_learn(df_tf_idf_stemed, 'df_tf_idf_stemed_none_pca', False)\n",
    "model_learn(df_tf_idf_lemitized, 'df_tf_idf_lemitized_pca', True)\n",
    "model_learn(df_tf_idf_lemitized, 'df_tf_idf_lemitized_none_pca', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37220e20-65f1-4e53-b1e3-38ea3109e2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_stemed_none_pca</th>\n",
       "      <td>0.949693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_stemed_none_pca</th>\n",
       "      <td>0.948943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_lemitized_trigram_none_pca</th>\n",
       "      <td>0.948567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_stemed_trigram_none_pca</th>\n",
       "      <td>0.948067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_lemitized_trigram_none_pca</th>\n",
       "      <td>0.947816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_stemed_trigram_none_pca</th>\n",
       "      <td>0.947065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_lemitized_none_pca</th>\n",
       "      <td>0.946440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_lemitized_none_pca</th>\n",
       "      <td>0.946189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_stemed_pca</th>\n",
       "      <td>0.937805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_stemed_pca</th>\n",
       "      <td>0.937179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_lemitized_trigram_min_df_many_features_pca</th>\n",
       "      <td>0.936178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_lemitized_pca</th>\n",
       "      <td>0.935803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_lemitized_pca</th>\n",
       "      <td>0.935552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_stemed_trigram_pca</th>\n",
       "      <td>0.935552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_stemed_trigram_pca</th>\n",
       "      <td>0.935552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_lemitized_trigram_min_df_pca</th>\n",
       "      <td>0.934051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_stemed_trigram_min_df_many_features_pca</th>\n",
       "      <td>0.933801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_stemed_trigram_min_df_none_pca</th>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_lemitized_trigram_min_df_none_pca</th>\n",
       "      <td>0.933050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_lemitized_trigram_pca</th>\n",
       "      <td>0.932674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_lemitized_trigram_min_df_many_features_none_pca</th>\n",
       "      <td>0.932549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_lemitized_trigram_pca</th>\n",
       "      <td>0.932174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_stemed_trigram_min_df_pca</th>\n",
       "      <td>0.931923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_lemitized_trigram_min_df_many_features_pca</th>\n",
       "      <td>0.930797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_stemed_trigram_min_df_many_features_pca</th>\n",
       "      <td>0.930672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_lemitized_trigram_min_df_many_features_none_pca</th>\n",
       "      <td>0.930547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_stemed_trigram_min_df_many_features_none_pca</th>\n",
       "      <td>0.930547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_lemitized_trigram_min_df_none_pca</th>\n",
       "      <td>0.930547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_stemed_trigram_min_df_many_features_none_pca</th>\n",
       "      <td>0.929295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_stemed_trigram_min_df_pca</th>\n",
       "      <td>0.927543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_cv_stemed_trigram_min_df_none_pca</th>\n",
       "      <td>0.927418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_task_params_df_tf_idf_lemitized_trigram_min_df_pca</th>\n",
       "      <td>0.927293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     metrics\n",
       "logreg_task_params_df_tf_idf_stemed_none_pca        0.949693\n",
       "logreg_task_params_df_cv_stemed_none_pca            0.948943\n",
       "logreg_task_params_df_cv_lemitized_trigram_none...  0.948567\n",
       "logreg_task_params_df_tf_idf_stemed_trigram_non...  0.948067\n",
       "logreg_task_params_df_tf_idf_lemitized_trigram_...  0.947816\n",
       "logreg_task_params_df_cv_stemed_trigram_none_pca    0.947065\n",
       "logreg_task_params_df_cv_lemitized_none_pca         0.946440\n",
       "logreg_task_params_df_tf_idf_lemitized_none_pca     0.946189\n",
       "logreg_task_params_df_tf_idf_stemed_pca             0.937805\n",
       "logreg_task_params_df_cv_stemed_pca                 0.937179\n",
       "logreg_task_params_df_cv_lemitized_trigram_min_...  0.936178\n",
       "logreg_task_params_df_cv_lemitized_pca              0.935803\n",
       "logreg_task_params_df_tf_idf_lemitized_pca          0.935552\n",
       "logreg_task_params_df_tf_idf_stemed_trigram_pca     0.935552\n",
       "logreg_task_params_df_cv_stemed_trigram_pca         0.935552\n",
       "logreg_task_params_df_cv_lemitized_trigram_min_...  0.934051\n",
       "logreg_task_params_df_tf_idf_stemed_trigram_min...  0.933801\n",
       "logreg_task_params_df_tf_idf_stemed_trigram_min...  0.933300\n",
       "logreg_task_params_df_cv_lemitized_trigram_min_...  0.933050\n",
       "logreg_task_params_df_cv_lemitized_trigram_pca      0.932674\n",
       "logreg_task_params_df_cv_lemitized_trigram_min_...  0.932549\n",
       "logreg_task_params_df_tf_idf_lemitized_trigram_pca  0.932174\n",
       "logreg_task_params_df_tf_idf_stemed_trigram_min...  0.931923\n",
       "logreg_task_params_df_tf_idf_lemitized_trigram_...  0.930797\n",
       "logreg_task_params_df_cv_stemed_trigram_min_df_...  0.930672\n",
       "logreg_task_params_df_tf_idf_lemitized_trigram_...  0.930547\n",
       "logreg_task_params_df_tf_idf_stemed_trigram_min...  0.930547\n",
       "logreg_task_params_df_tf_idf_lemitized_trigram_...  0.930547\n",
       "logreg_task_params_df_cv_stemed_trigram_min_df_...  0.929295\n",
       "logreg_task_params_df_cv_stemed_trigram_min_df_pca  0.927543\n",
       "logreg_task_params_df_cv_stemed_trigram_min_df_...  0.927418\n",
       "logreg_task_params_df_tf_idf_lemitized_trigram_...  0.927293"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(result, orient='index',columns=['metrics'])\n",
    "df.sort_values(by=['metrics'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd6744-4639-42cb-ae34-1c14aeff7680",
   "metadata": {},
   "source": [
    "Вывод : Колличиство фичей влияет на качество , однако необходимо соблюдать баланс между фичами которые очень часто или редко встречаются . Использование PCA снижает точность модели однако уменьшает количество признаков для обучения что ускоряет скорость обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babeb6eb-49d9-4216-8fd8-c79f6c6df25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
